{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP76IXf/XyURWTelq2NR9PS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nik8839/deep_learning_project/blob/trial_1/vit_increased_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdftAwme61Nb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras.applications import Vit\n",
        "\n",
        "# Rest of the code\n",
        "\n",
        "from PIL import Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6jBtOUp7EcP",
        "outputId": "0f82c2b1-82d3-4a94-f548-52d3f29987e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load your image data\n",
        "classes = []\n",
        "data = []\n",
        "Y = []\n",
        "image_folder_path = '/content/drive/MyDrive/dataset'  # Change this to the path where your folders are extracted\n",
        "folders = os.listdir(image_folder_path)\n",
        "count = 0\n",
        "for folder in folders:\n",
        "    classes.append(folder)\n",
        "    folder_path = os.path.join(image_folder_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"Reading images from {folder}\")\n",
        "        images = os.listdir(folder_path)\n",
        "        for image_name in images:\n",
        "            Y.append(count)\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize((112,112), resample=Image.LANCZOS).convert('RGB')\n",
        "            image_array = np.asarray(image)\n",
        "            data.append(image_array)\n",
        "\n",
        "        count += 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSThpSZ7PMs",
        "outputId": "0259e5df-75cc-41e5-e786-4418625ab1b7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading images from lightning\n",
            "Reading images from rime\n",
            "Reading images from glaze\n",
            "Reading images from rainbow\n",
            "Reading images from hail\n",
            "Reading images from frost\n",
            "Reading images from sandstorm\n",
            "Reading images from rain\n",
            "Reading images from snow\n",
            "Reading images from fogsmog\n",
            "Reading images from dew\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to numpy arrays\n",
        "data = np.array(data)\n",
        "Y = np.array(Y)\n"
      ],
      "metadata": {
        "id": "59fAt9XE7aGU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "QArvOY6h7b0T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vit_keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYor4-Ez8I9C",
        "outputId": "fa7d27ae-9c34-4ca6-bf0b-b630382fded2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit_keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi48gtHx8S0v",
        "outputId": "92e12d72-4e6b-4a3b-ace9-43ca011982b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from vit_keras import vit, utils\n",
        "\n",
        "# Assuming you have already loaded and preprocessed your data (X_train, Y_train, X_val, Y_val)\n",
        "\n",
        "# Define the Vision Transformer model\n",
        "def create_vit_model(image_size, num_classes):\n",
        "    model = vit.vit_b16(\n",
        "        image_size=image_size,\n",
        "        activation='softmax',\n",
        "        pretrained=True,  # Set to True if you want to use pre-trained weights\n",
        "        include_top=True,\n",
        "        classes=num_classes,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create the ViT model\n",
        "input_shape = (112,112)\n",
        "num_classes = 11  # Adjust based on your specific classification task\n",
        "vit_model = create_vit_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "vit_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "vit_model.summary()\n",
        "\n",
        "# Train the model\n",
        "vit_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=25)\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = vit_model.evaluate(X_val, Y_val)\n",
        "print(\"Validation Loss:\", eval_result[0])\n",
        "print(\"Validation Accuracy:\", eval_result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQgCnoY7ls7",
        "outputId": "1cbd0454-a9c3-4d3d-8399-4584e4ff66df"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 7, 7\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vit-b16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
            "                                                                 \n",
            " embedding (Conv2D)          (None, 7, 7, 768)         590592    \n",
            "                                                                 \n",
            " reshape_7 (Reshape)         (None, 49, 768)           0         \n",
            "                                                                 \n",
            " class_token (ClassToken)    (None, 50, 768)           768       \n",
            "                                                                 \n",
            " Transformer/posembed_input  (None, 50, 768)           38400     \n",
            "  (AddPositionEmbs)                                              \n",
            "                                                                 \n",
            " Transformer/encoderblock_0  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_2  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_3  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_4  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_5  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_6  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_7  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_8  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_9  ((None, 50, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 50, 768),         7087872   \n",
            " 0 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 50, 768),         7087872   \n",
            " 1 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoder_norm (  (None, 50, 768)           1536      \n",
            " LayerNormalization)                                             \n",
            "                                                                 \n",
            " ExtractToken (Lambda)       (None, 768)               0         \n",
            "                                                                 \n",
            " head (Dense)                (None, 1000)              769000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86454760 (329.80 MB)\n",
            "Trainable params: 86454760 (329.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "221/221 [==============================] - 117s 353ms/step - loss: 1.9038 - accuracy: 0.3739 - val_loss: 1.6517 - val_accuracy: 0.4193\n",
            "Epoch 2/20\n",
            "221/221 [==============================] - 72s 328ms/step - loss: 1.3523 - accuracy: 0.5355 - val_loss: 1.3142 - val_accuracy: 0.5654\n",
            "Epoch 3/20\n",
            "221/221 [==============================] - 72s 324ms/step - loss: 1.2385 - accuracy: 0.5741 - val_loss: 1.2335 - val_accuracy: 0.5719\n",
            "Epoch 4/20\n",
            "221/221 [==============================] - 66s 299ms/step - loss: 1.1167 - accuracy: 0.6193 - val_loss: 1.2606 - val_accuracy: 0.5661\n",
            "Epoch 5/20\n",
            "221/221 [==============================] - 71s 323ms/step - loss: 1.0771 - accuracy: 0.6250 - val_loss: 1.4615 - val_accuracy: 0.5240\n",
            "Epoch 6/20\n",
            "221/221 [==============================] - 67s 302ms/step - loss: 1.0866 - accuracy: 0.6319 - val_loss: 1.1840 - val_accuracy: 0.5945\n",
            "Epoch 7/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 1.0094 - accuracy: 0.6508 - val_loss: 1.0993 - val_accuracy: 0.6330\n",
            "Epoch 8/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 0.9769 - accuracy: 0.6592 - val_loss: 1.1360 - val_accuracy: 0.6221\n",
            "Epoch 9/20\n",
            "221/221 [==============================] - 71s 323ms/step - loss: 0.9562 - accuracy: 0.6702 - val_loss: 1.2074 - val_accuracy: 0.5770\n",
            "Epoch 10/20\n",
            "221/221 [==============================] - 72s 324ms/step - loss: 0.9131 - accuracy: 0.6850 - val_loss: 1.3729 - val_accuracy: 0.5538\n",
            "Epoch 11/20\n",
            "221/221 [==============================] - 71s 323ms/step - loss: 0.9332 - accuracy: 0.6790 - val_loss: 1.1413 - val_accuracy: 0.6032\n",
            "Epoch 12/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 0.9040 - accuracy: 0.6957 - val_loss: 1.1159 - val_accuracy: 0.6192\n",
            "Epoch 13/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 0.8655 - accuracy: 0.7017 - val_loss: 1.1735 - val_accuracy: 0.6112\n",
            "Epoch 14/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 0.9424 - accuracy: 0.6788 - val_loss: 1.2307 - val_accuracy: 0.5887\n",
            "Epoch 15/20\n",
            "221/221 [==============================] - 71s 324ms/step - loss: 0.8254 - accuracy: 0.7199 - val_loss: 1.1042 - val_accuracy: 0.6374\n",
            "Epoch 16/20\n",
            "221/221 [==============================] - 72s 324ms/step - loss: 0.7922 - accuracy: 0.7271 - val_loss: 1.1974 - val_accuracy: 0.6090\n",
            "Epoch 17/20\n",
            "221/221 [==============================] - 71s 323ms/step - loss: 0.7725 - accuracy: 0.7335 - val_loss: 1.2494 - val_accuracy: 0.5916\n",
            "Epoch 18/20\n",
            "221/221 [==============================] - 72s 324ms/step - loss: 0.7672 - accuracy: 0.7282 - val_loss: 1.1677 - val_accuracy: 0.6315\n",
            "Epoch 19/20\n",
            "221/221 [==============================] - 66s 300ms/step - loss: 0.8265 - accuracy: 0.7181 - val_loss: 1.2056 - val_accuracy: 0.5996\n",
            "Epoch 20/20\n",
            "221/221 [==============================] - 71s 323ms/step - loss: 0.7592 - accuracy: 0.7368 - val_loss: 1.0972 - val_accuracy: 0.6519\n",
            "43/43 [==============================] - 5s 122ms/step - loss: 1.0972 - accuracy: 0.6519\n",
            "Validation Loss: 1.0971542596817017\n",
            "Validation Accuracy: 0.6518895626068115\n"
          ]
        }
      ]
    }
  ]
}