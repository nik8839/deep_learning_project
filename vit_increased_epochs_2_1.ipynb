{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfEtoOMCPmMfK2rVEt9+i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nik8839/deep_learning_project/blob/trial_1/vit_increased_epochs_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdftAwme61Nb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras.applications import Vit\n",
        "\n",
        "# Rest of the code\n",
        "\n",
        "from PIL import Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6jBtOUp7EcP",
        "outputId": "66bf8bf4-9f0e-4e14-ea63-056ac6c3d83c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load your image data\n",
        "classes = []\n",
        "data = []\n",
        "Y = []\n",
        "image_folder_path = '/content/drive/MyDrive/dataset'  # Change this to the path where your folders are extracted\n",
        "folders = os.listdir(image_folder_path)\n",
        "count = 0\n",
        "for folder in folders:\n",
        "    classes.append(folder)\n",
        "    folder_path = os.path.join(image_folder_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"Reading images from {folder}\")\n",
        "        images = os.listdir(folder_path)\n",
        "        for image_name in images:\n",
        "            Y.append(count)\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize((144,144), resample=Image.LANCZOS).convert('RGB')\n",
        "            image_array = np.asarray(image)\n",
        "            data.append(image_array)\n",
        "\n",
        "        count += 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSThpSZ7PMs",
        "outputId": "82972996-9830-4ae9-c50c-b3fe32e45c0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading images from lightning\n",
            "Reading images from rime\n",
            "Reading images from glaze\n",
            "Reading images from rainbow\n",
            "Reading images from hail\n",
            "Reading images from frost\n",
            "Reading images from sandstorm\n",
            "Reading images from rain\n",
            "Reading images from snow\n",
            "Reading images from fogsmog\n",
            "Reading images from dew\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to numpy arrays\n",
        "data = np.array(data)\n",
        "Y = np.array(Y)\n"
      ],
      "metadata": {
        "id": "59fAt9XE7aGU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "QArvOY6h7b0T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vit_keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYor4-Ez8I9C",
        "outputId": "0cdff14d-c185-41b5-b14a-5c71751bb34f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit_keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.3)\n",
            "Collecting validators (from vit_keras)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.23.5)\n",
            "Installing collected packages: validators, vit_keras\n",
            "Successfully installed validators-0.22.0 vit_keras-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi48gtHx8S0v",
        "outputId": "e8168bd4-47bc-463f-f381-dab43a755643"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/612.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m440.3/612.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.22.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from vit_keras import vit, utils\n",
        "\n",
        "# Assuming you have already loaded and preprocessed your data (X_train, Y_train, X_val, Y_val)\n",
        "\n",
        "# Define the Vision Transformer model\n",
        "def create_vit_model(image_size, num_classes):\n",
        "    model = vit.vit_b16(\n",
        "        image_size=image_size,\n",
        "        activation='softmax',\n",
        "        #pretrained=False,  # Set to True if you want to use pre-trained weights\n",
        "        #include_top=True,\n",
        "        classes=num_classes,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create the ViT model\n",
        "input_shape = (144,144)\n",
        "num_classes = 11  # Adjust based on your specific classification task\n",
        "vit_model = create_vit_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "vit_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "vit_model.summary()\n",
        "\n",
        "# Train the model\n",
        "vit_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=20)\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = vit_model.evaluate(X_val, Y_val)\n",
        "print(\"Validation Loss:\", eval_result[0])\n",
        "print(\"Validation Accuracy:\", eval_result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQgCnoY7ls7",
        "outputId": "e0c8d08f-5a41-46fc-dd2f-57409b94e34b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
            "347502902/347502902 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 9, 9\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vit-b16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 144, 144, 3)]     0         \n",
            "                                                                 \n",
            " embedding (Conv2D)          (None, 9, 9, 768)         590592    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 81, 768)           0         \n",
            "                                                                 \n",
            " class_token (ClassToken)    (None, 82, 768)           768       \n",
            "                                                                 \n",
            " Transformer/posembed_input  (None, 82, 768)           62976     \n",
            "  (AddPositionEmbs)                                              \n",
            "                                                                 \n",
            " Transformer/encoderblock_0  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_2  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_3  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_4  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_5  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_6  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_7  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_8  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_9  ((None, 82, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 82, 768),         7087872   \n",
            " 0 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 82, 768),         7087872   \n",
            " 1 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoder_norm (  (None, 82, 768)           1536      \n",
            " LayerNormalization)                                             \n",
            "                                                                 \n",
            " ExtractToken (Lambda)       (None, 768)               0         \n",
            "                                                                 \n",
            " head (Dense)                (None, 1000)              769000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86479336 (329.89 MB)\n",
            "Trainable params: 86479336 (329.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "276/276 [==============================] - 161s 394ms/step - loss: 1.8121 - accuracy: 0.4054 - val_loss: 1.7288 - val_accuracy: 0.4230\n",
            "Epoch 2/20\n",
            "276/276 [==============================] - 106s 385ms/step - loss: 1.2959 - accuracy: 0.5564 - val_loss: 1.3493 - val_accuracy: 0.5291\n",
            "Epoch 3/20\n",
            "276/276 [==============================] - 106s 385ms/step - loss: 1.1797 - accuracy: 0.5977 - val_loss: 1.2143 - val_accuracy: 0.5872\n",
            "Epoch 4/20\n",
            "276/276 [==============================] - 106s 385ms/step - loss: 1.1337 - accuracy: 0.6064 - val_loss: 1.2302 - val_accuracy: 0.5814\n",
            "Epoch 5/20\n",
            "276/276 [==============================] - 108s 393ms/step - loss: 1.0672 - accuracy: 0.6268 - val_loss: 1.1217 - val_accuracy: 0.6185\n",
            "Epoch 6/20\n",
            "276/276 [==============================] - 108s 393ms/step - loss: 1.0398 - accuracy: 0.6413 - val_loss: 1.0334 - val_accuracy: 0.6352\n",
            "Epoch 7/20\n",
            "276/276 [==============================] - 107s 386ms/step - loss: 1.0042 - accuracy: 0.6542 - val_loss: 1.1542 - val_accuracy: 0.6105\n",
            "Epoch 8/20\n",
            "276/276 [==============================] - 108s 393ms/step - loss: 1.0056 - accuracy: 0.6512 - val_loss: 1.0811 - val_accuracy: 0.6243\n",
            "Epoch 9/20\n",
            "276/276 [==============================] - 108s 392ms/step - loss: 0.9639 - accuracy: 0.6702 - val_loss: 1.2067 - val_accuracy: 0.5952\n",
            "Epoch 10/20\n",
            "276/276 [==============================] - 106s 385ms/step - loss: 0.9577 - accuracy: 0.6633 - val_loss: 1.0741 - val_accuracy: 0.6265\n",
            "Epoch 11/20\n",
            "276/276 [==============================] - 106s 383ms/step - loss: 0.9312 - accuracy: 0.6793 - val_loss: 1.1686 - val_accuracy: 0.6250\n",
            "Epoch 12/20\n",
            "276/276 [==============================] - 108s 391ms/step - loss: 0.9038 - accuracy: 0.6855 - val_loss: 1.1464 - val_accuracy: 0.6090\n",
            "Epoch 13/20\n",
            "276/276 [==============================] - 108s 392ms/step - loss: 0.9048 - accuracy: 0.6870 - val_loss: 1.0307 - val_accuracy: 0.6439\n",
            "Epoch 14/20\n",
            "276/276 [==============================] - 108s 391ms/step - loss: 0.8544 - accuracy: 0.7062 - val_loss: 1.0694 - val_accuracy: 0.6381\n",
            "Epoch 15/20\n",
            "276/276 [==============================] - 108s 391ms/step - loss: 0.8297 - accuracy: 0.7142 - val_loss: 1.1085 - val_accuracy: 0.6265\n",
            "Epoch 16/20\n",
            "276/276 [==============================] - 108s 391ms/step - loss: 0.8357 - accuracy: 0.7173 - val_loss: 1.1254 - val_accuracy: 0.6286\n",
            "Epoch 17/20\n",
            "276/276 [==============================] - 106s 383ms/step - loss: 0.8425 - accuracy: 0.7108 - val_loss: 1.1093 - val_accuracy: 0.6235\n",
            "Epoch 18/20\n",
            "276/276 [==============================] - 106s 383ms/step - loss: 0.8034 - accuracy: 0.7328 - val_loss: 1.2651 - val_accuracy: 0.5778\n",
            "Epoch 19/20\n",
            "276/276 [==============================] - 108s 390ms/step - loss: 0.7867 - accuracy: 0.7271 - val_loss: 1.1405 - val_accuracy: 0.6272\n",
            "Epoch 20/20\n",
            "276/276 [==============================] - 108s 389ms/step - loss: 0.7996 - accuracy: 0.7186 - val_loss: 1.1686 - val_accuracy: 0.6286\n",
            "43/43 [==============================] - 8s 187ms/step - loss: 1.1686 - accuracy: 0.6286\n",
            "Validation Loss: 1.1685872077941895\n",
            "Validation Accuracy: 0.6286337375640869\n"
          ]
        }
      ]
    }
  ]
}