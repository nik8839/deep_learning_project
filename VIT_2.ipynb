{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpLBLGWeE2EPg1xbYTBgLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nik8839/deep_learning_project/blob/trial_1/VIT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdftAwme61Nb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras.applications import Vit\n",
        "\n",
        "# Rest of the code\n",
        "\n",
        "from PIL import Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6jBtOUp7EcP",
        "outputId": "0f82c2b1-82d3-4a94-f548-52d3f29987e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load your image data\n",
        "classes = []\n",
        "data = []\n",
        "Y = []\n",
        "image_folder_path = '/content/drive/MyDrive/dataset'  # Change this to the path where your folders are extracted\n",
        "folders = os.listdir(image_folder_path)\n",
        "count = 0\n",
        "for folder in folders:\n",
        "    classes.append(folder)\n",
        "    folder_path = os.path.join(image_folder_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"Reading images from {folder}\")\n",
        "        images = os.listdir(folder_path)\n",
        "        for image_name in images:\n",
        "            Y.append(count)\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize((128,128), resample=Image.LANCZOS).convert('RGB')\n",
        "            image_array = np.asarray(image)\n",
        "            data.append(image_array)\n",
        "\n",
        "        count += 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSThpSZ7PMs",
        "outputId": "08cf38ce-639f-400a-e535-821f902cca3b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading images from lightning\n",
            "Reading images from rime\n",
            "Reading images from glaze\n",
            "Reading images from rainbow\n",
            "Reading images from hail\n",
            "Reading images from frost\n",
            "Reading images from sandstorm\n",
            "Reading images from rain\n",
            "Reading images from snow\n",
            "Reading images from fogsmog\n",
            "Reading images from dew\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to numpy arrays\n",
        "data = np.array(data)\n",
        "Y = np.array(Y)\n"
      ],
      "metadata": {
        "id": "59fAt9XE7aGU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(data, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "QArvOY6h7b0T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vit_keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYor4-Ez8I9C",
        "outputId": "fa7d27ae-9c34-4ca6-bf0b-b630382fded2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit_keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi48gtHx8S0v",
        "outputId": "92e12d72-4e6b-4a3b-ace9-43ca011982b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from vit_keras import vit, utils\n",
        "\n",
        "# Assuming you have already loaded and preprocessed your data (X_train, Y_train, X_val, Y_val)\n",
        "\n",
        "# Define the Vision Transformer model\n",
        "def create_vit_model(image_size, num_classes):\n",
        "    model = vit.vit_b16(\n",
        "        image_size=image_size,\n",
        "        activation='softmax',\n",
        "        pretrained=True,  # Set to True if you want to use pre-trained weights\n",
        "        include_top=True,\n",
        "        classes=num_classes,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create the ViT model\n",
        "input_shape = (128,128)\n",
        "num_classes = 11  # Adjust based on your specific classification task\n",
        "vit_model = create_vit_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "vit_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "vit_model.summary()\n",
        "\n",
        "# Train the model\n",
        "vit_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = vit_model.evaluate(X_val, Y_val)\n",
        "print(\"Validation Loss:\", eval_result[0])\n",
        "print(\"Validation Accuracy:\", eval_result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQgCnoY7ls7",
        "outputId": "aae969de-09fe-4d80-a7ba-dc3d559dd6bd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vit-b16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " embedding (Conv2D)          (None, 8, 8, 768)         590592    \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 64, 768)           0         \n",
            "                                                                 \n",
            " class_token (ClassToken)    (None, 65, 768)           768       \n",
            "                                                                 \n",
            " Transformer/posembed_input  (None, 65, 768)           49920     \n",
            "  (AddPositionEmbs)                                              \n",
            "                                                                 \n",
            " Transformer/encoderblock_0  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_2  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_3  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_4  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_5  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_6  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_7  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_8  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_9  ((None, 65, 768),         7087872   \n",
            "  (TransformerBlock)          (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
            " 0 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
            " 1 (TransformerBlock)         (None, 12, None, None)             \n",
            "                             )                                   \n",
            "                                                                 \n",
            " Transformer/encoder_norm (  (None, 65, 768)           1536      \n",
            " LayerNormalization)                                             \n",
            "                                                                 \n",
            " ExtractToken (Lambda)       (None, 768)               0         \n",
            "                                                                 \n",
            " head (Dense)                (None, 1000)              769000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86466280 (329.84 MB)\n",
            "Trainable params: 86466280 (329.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "172/172 [==============================] - 127s 510ms/step - loss: 1.9280 - accuracy: 0.3714 - val_loss: 1.4899 - val_accuracy: 0.5058\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 82s 480ms/step - loss: 1.2966 - accuracy: 0.5597 - val_loss: 1.2042 - val_accuracy: 0.5828\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 86s 503ms/step - loss: 1.1270 - accuracy: 0.6084 - val_loss: 1.2695 - val_accuracy: 0.5625\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 87s 506ms/step - loss: 1.0522 - accuracy: 0.6372 - val_loss: 1.1259 - val_accuracy: 0.6235\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 87s 505ms/step - loss: 0.9596 - accuracy: 0.6655 - val_loss: 1.1039 - val_accuracy: 0.6199\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 83s 480ms/step - loss: 0.9022 - accuracy: 0.6857 - val_loss: 1.1006 - val_accuracy: 0.6185\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 87s 505ms/step - loss: 0.8768 - accuracy: 0.6950 - val_loss: 1.0918 - val_accuracy: 0.6381\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 87s 505ms/step - loss: 0.8277 - accuracy: 0.7168 - val_loss: 1.1591 - val_accuracy: 0.6076\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 83s 481ms/step - loss: 0.7666 - accuracy: 0.7390 - val_loss: 1.2197 - val_accuracy: 0.6112\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 83s 481ms/step - loss: 0.7445 - accuracy: 0.7462 - val_loss: 1.0830 - val_accuracy: 0.6424\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 83s 480ms/step - loss: 0.7319 - accuracy: 0.7491 - val_loss: 1.0746 - val_accuracy: 0.6512\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 83s 480ms/step - loss: 0.6939 - accuracy: 0.7642 - val_loss: 1.1733 - val_accuracy: 0.6272\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 87s 504ms/step - loss: 0.6904 - accuracy: 0.7628 - val_loss: 1.1222 - val_accuracy: 0.6483\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 82s 479ms/step - loss: 0.6950 - accuracy: 0.7642 - val_loss: 1.2150 - val_accuracy: 0.5836\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 87s 504ms/step - loss: 0.6568 - accuracy: 0.7677 - val_loss: 1.1521 - val_accuracy: 0.6199\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 87s 505ms/step - loss: 0.6128 - accuracy: 0.7957 - val_loss: 1.2380 - val_accuracy: 0.6163\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 82s 479ms/step - loss: 0.6514 - accuracy: 0.7715 - val_loss: 1.1637 - val_accuracy: 0.6483\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 82s 479ms/step - loss: 0.6138 - accuracy: 0.7904 - val_loss: 1.0717 - val_accuracy: 0.6526\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 87s 504ms/step - loss: 0.6035 - accuracy: 0.7931 - val_loss: 1.1243 - val_accuracy: 0.6628\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 87s 505ms/step - loss: 0.5810 - accuracy: 0.7966 - val_loss: 1.0963 - val_accuracy: 0.6562\n",
            "43/43 [==============================] - 6s 139ms/step - loss: 1.0963 - accuracy: 0.6562\n",
            "Validation Loss: 1.0963432788848877\n",
            "Validation Accuracy: 0.65625\n"
          ]
        }
      ]
    }
  ]
}